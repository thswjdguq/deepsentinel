from fastapi import FastAPI, File, UploadFile, HTTPException, WebSocket, WebSocketDisconnect
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from typing import Optional, Dict
import uvicorn
import time
import random
import os
from dotenv import load_dotenv
from openai import OpenAI
import asyncio
import json

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

app = FastAPI(
    title="DeepSentinel AI Server",
    description="ë”¥í˜ì´í¬ íƒì§€ AI ë¶„ì„ ì„œë²„",
    version="2.1.0"
)

# CORS ì„¤ì •
app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:3000", "http://localhost:5000"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


class AnalysisMetrics(BaseModel):
    """
    ë¶„ì„ ìˆ˜ì¹˜ ëª¨ë¸
    """
    eye_blink_rate: float  # ëˆˆ ê¹œë¹¡ì„ ë¹ˆë„ (0-100)
    lip_sync_score: float  # ì…ìˆ  ë™ê¸°í™” ì ìˆ˜ (0-100)
    lighting_consistency: float  # ë¹›ì˜ ì¼ê´€ì„± (0-100)
    facial_artifacts: float  # ì–¼êµ´ ì¸ê³µë¬¼ ì§€ìˆ˜ (0-100)
    texture_quality: float  # í…ìŠ¤ì²˜ í’ˆì§ˆ (0-100)
    motion_smoothness: float  # ë™ì‘ ë¶€ë“œëŸ¬ì›€ (0-100)


class AnalysisResponse(BaseModel):
    """
    ë¶„ì„ ê²°ê³¼ ì‘ë‹µ ëª¨ë¸
    """
    result: str  # 'real', 'fake', 'uncertain'
    confidence: float  # ì‹ ë¢°ë„ (0-1)
    metrics: AnalysisMetrics
    report: str  # GPT ìƒì„± ë¦¬í¬íŠ¸
    analysis_time: float  # ë¶„ì„ ì†Œìš” ì‹œê°„ (ì´ˆ)


class URLAnalysisRequest(BaseModel):
    """
    URL ë¶„ì„ ìš”ì²­ ëª¨ë¸
    """
    url: str
    platform: Optional[str] = None  # 'youtube', 'instagram', 'tiktok' ë“±


def generate_analysis_metrics() -> AnalysisMetrics:
    """
    ë”¥í˜ì´í¬ ë¶„ì„ì„ ìœ„í•œ ì„ì˜ì˜ ë©”íŠ¸ë¦­ì„ ìƒì„±í•©ë‹ˆë‹¤.
    ì‹¤ì œ í™˜ê²½ì—ì„œëŠ” ë”¥ëŸ¬ë‹ ëª¨ë¸ì˜ ì¶œë ¥ìœ¼ë¡œ ëŒ€ì²´ë©ë‹ˆë‹¤.
    
    Returns:
        AnalysisMetrics: ìƒì„±ëœ ë¶„ì„ ìˆ˜ì¹˜
    """
    # ì§„ì§œ/ê°€ì§œë¥¼ ê²°ì •í•˜ëŠ” ëœë¤ ì‹œë“œ
    is_fake = random.random() < 0.4  # 40% í™•ë¥ ë¡œ ë”¥í˜ì´í¬
    
    if is_fake:
        # ë”¥í˜ì´í¬ì¼ ê²½ìš°: ë¹„ì •ìƒì ì¸ ìˆ˜ì¹˜
        metrics = AnalysisMetrics(
            eye_blink_rate=random.uniform(15, 35),  # ë¹„ì •ìƒì ìœ¼ë¡œ ë‚®ìŒ
            lip_sync_score=random.uniform(50, 75),  # ë™ê¸°í™” ë¬¸ì œ
            lighting_consistency=random.uniform(40, 70),  # ë¹›ì˜ ë¶ˆì¼ì¹˜
            facial_artifacts=random.uniform(60, 90),  # ë†’ì€ ì¸ê³µë¬¼
            texture_quality=random.uniform(45, 70),  # ë‚®ì€ í…ìŠ¤ì²˜ í’ˆì§ˆ
            motion_smoothness=random.uniform(50, 75),  # ë¶€ìì—°ìŠ¤ëŸ¬ìš´ ë™ì‘
        )
    else:
        # ì§„ì§œì¼ ê²½ìš°: ì •ìƒ ìˆ˜ì¹˜
        metrics = AnalysisMetrics(
            eye_blink_rate=random.uniform(65, 95),  # ìì—°ìŠ¤ëŸ¬ìš´ ëˆˆ ê¹œë¹¡ì„
            lip_sync_score=random.uniform(80, 98),  # ì™„ë²½í•œ ë™ê¸°í™”
            lighting_consistency=random.uniform(75, 95),  # ì¼ê´€ëœ ë¹›
            facial_artifacts=random.uniform(5, 25),  # ë‚®ì€ ì¸ê³µë¬¼
            texture_quality=random.uniform(80, 95),  # ë†’ì€ í…ìŠ¤ì²˜ í’ˆì§ˆ
            motion_smoothness=random.uniform(85, 98),  # ìì—°ìŠ¤ëŸ¬ìš´ ë™ì‘
        )
    
    return metrics


def calculate_overall_score(metrics: AnalysisMetrics) -> tuple[str, float]:
    """
    ë©”íŠ¸ë¦­ì„ ê¸°ë°˜ìœ¼ë¡œ ì „ì²´ ì ìˆ˜ì™€ ê²°ê³¼ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.
    
    Args:
        metrics: ë¶„ì„ ë©”íŠ¸ë¦­
        
    Returns:
        tuple: (result, confidence) - ê²°ê³¼ì™€ ì‹ ë¢°ë„
    """
    # ê°€ì¤‘ì¹˜ ì ìš© ì ìˆ˜ ê³„ì‚°
    score = (
        metrics.eye_blink_rate * 0.15 +
        metrics.lip_sync_score * 0.25 +
        metrics.lighting_consistency * 0.20 +
        (100 - metrics.facial_artifacts) * 0.25 +
        metrics.texture_quality * 0.10 +
        metrics.motion_smoothness * 0.05
    )
    
    # ê²°ê³¼ íŒì •
    if score >= 75:
        result = "real"
        confidence = min(score / 100, 0.95)
    elif score >= 50:
        result = "uncertain"
        confidence = 0.5 + (score - 50) / 100
    else:
        result = "fake"
        confidence = min((100 - score) / 100, 0.95)
    
    return result, confidence


async def generate_gpt_report(
    metrics: AnalysisMetrics,
    result: str,
    confidence: float
) -> str:
    """
    OpenAI GPT-4o-minië¥¼ ì‚¬ìš©í•˜ì—¬ ì „ë¬¸ì ì¸ ë¶„ì„ ë¦¬í¬íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    
    Args:
        metrics: ë¶„ì„ ë©”íŠ¸ë¦­
        result: ë¶„ì„ ê²°ê³¼ ('real', 'fake', 'uncertain')
        confidence: ì‹ ë¢°ë„ (0-1)
        
    Returns:
        str: ìƒì„±ëœ ë¦¬í¬íŠ¸
    """
    # ê²°ê³¼ë³„ ë ˆì´ë¸”
    result_labels = {
        "real": "ì•ˆì „",
        "fake": "ìœ„í—˜",
        "uncertain": "ì£¼ì˜"
    }
    
    # GPT í”„ë¡¬í”„íŠ¸ êµ¬ì„±
    prompt = f"""ëŒ€í•œë¯¼êµ­ êµ­ê°€ì •ë³´ì› ì‚°í•˜ ë””ì§€í„¸ í¬ë Œì‹ ì—°êµ¬ì†Œ ìˆ˜ì„ ë¶„ì„ê´€ìœ¼ë¡œì„œ, ë‹¤ìŒ ì˜ìƒì— ëŒ€í•œ ê³µì‹ ê°ì • ë³´ê³ ì„œë¥¼ ì‘ì„±í•˜ì‹­ì‹œì˜¤.

[ì˜ìƒ ê°ì • ê²°ê³¼]
íŒì •: {result_labels[result]} ({confidence*100:.1f}%)

[ì„¸ë¶€ ë¶„ì„ ì§€í‘œ]
1. ëˆˆ ê¹œë¹¡ì„ ìì—°ë„ (EAR): {metrics.eye_blink_rate:.1f}/100
2. ìŒì„±-ì…ìˆ  ë™ê¸°í™” ì •ë°€ë„ (MAR): {metrics.lip_sync_score:.1f}/100
3. ì¡°ëª… ì¼ê´€ì„± ë¶„ì„: {metrics.lighting_consistency:.1f}/100
4. ì–¼êµ´ ì•„í‹°íŒ©íŠ¸ ê²€ì¶œ: {metrics.facial_artifacts:.1f}/100
5. í…ìŠ¤ì²˜ í’ˆì§ˆ í‰ê°€: {metrics.texture_quality:.1f}/100
6. ëª¨ì…˜ ìì—°ë„ ë¶„ì„: {metrics.motion_smoothness:.1f}/100

[ë³´ê³ ì„œ ì‘ì„± ì§€ì¹¨]
- 3~4ë¬¸ì¥ìœ¼ë¡œ ê°„ê²°í•˜ê²Œ ì‘ì„±
- ë²•ì • ì œì¶œ ê°€ëŠ¥í•œ ê°ê´€ì  í‘œí˜„ ì‚¬ìš©
- "ë³¸ ì˜ìƒì€ [{result_labels[result]}] íŒì •ì„ ë°›ì•˜ìœ¼ë©°"ë¡œ ì‹œì‘
- í•µì‹¬ ê·¼ê±° ìˆ˜ì¹˜ë¥¼ ëª…í™•íˆ ì œì‹œ
- ê³µì‹ ë³´ê³ ì„œ ì–´íˆ¬ (í•˜ì‹­ì‹œì˜¤ì²´)
- ì „ë¬¸ ìš©ì–´ ì‚¬ìš© ì‹œ ê´„í˜¸ ì•ˆì— ì˜ë¬¸ ë³‘ê¸°

[ê°ì • ì˜ê²¬]:"""

    try:
        # OpenAI API í˜¸ì¶œ
        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {
                    "role": "system",
                    "content": """ë‹¹ì‹ ì€ ëŒ€í•œë¯¼êµ­ êµ­ê°€ì •ë³´ì› ì‚°í•˜ ë””ì§€í„¸ í¬ë Œì‹ ì—°êµ¬ì†Œì˜ ìˆ˜ì„ ë¶„ì„ê´€ì…ë‹ˆë‹¤. 
                    
ê·€í•˜ì˜ ë³´ê³ ì„œëŠ” ë²•ì • ì¦ê±°ë¡œ ì‚¬ìš©ë  ìˆ˜ ìˆìœ¼ë©°, ë‹¤ìŒ ì›ì¹™ì„ ì¤€ìˆ˜í•´ì•¼ í•©ë‹ˆë‹¤:
1. ê°ê´€ì ì´ê³  ë‹¨í˜¸í•œ ì–´ì¡°
2. ê³µì‹ ê°ì •ì„œ í˜•ì‹ ì¤€ìˆ˜
3. ì „ë¬¸ ê¸°ìˆ  ìš©ì–´ ì •í™•íˆ ì‚¬ìš©
4. ìˆ˜ì¹˜ ê¸°ë°˜ì˜ ë…¼ë¦¬ì  ê·¼ê±° ì œì‹œ
5. í•˜ì‹­ì‹œì˜¤ì²´ ì‚¬ìš©

ë³´ê³ ì„œëŠ” ê²€ì°°, ë²•ì›, ì •ë³´ê¸°ê´€ì— ì œì¶œë  ìˆ˜ ìˆìœ¼ë¯€ë¡œ ìµœê³  ìˆ˜ì¤€ì˜ ì „ë¬¸ì„±ê³¼ ì‹ ë¢°ì„±ì„ ìœ ì§€í•˜ì‹­ì‹œì˜¤."""
                },
                {
                    "role": "user",
                    "content": prompt
                }
            ],
            temperature=0.5,  # ë” ê°ê´€ì ì´ê³  ì¼ê´€ëœ ë‹µë³€ì„ ìœ„í•´ ë‚®ì¶¤
            max_tokens=350
        )
        
        report = response.choices[0].message.content.strip()
        return report
        
    except Exception as e:
        # OpenAI API í˜¸ì¶œ ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ ë¦¬í¬íŠ¸ ë°˜í™˜
        print(f"OpenAI API Error: {e}")
        return f"""ë³¸ ì˜ìƒì€ [{result_labels[result]}] íŒì •ì„ ë°›ì•˜ìœ¼ë©°, ë¶„ì„ ì‹ ë¢°ë„ëŠ” {confidence*100:.1f}%ì…ë‹ˆë‹¤. ì£¼ìš” ê°ì • ê·¼ê±°ë¡œëŠ” ìŒì„±-ì…ìˆ  ë™ê¸°í™”(MAR) {metrics.lip_sync_score:.1f}ì , ì¡°ëª… ì¼ê´€ì„± ë¶„ì„ {metrics.lighting_consistency:.1f}ì , ì–¼êµ´ ì•„í‹°íŒ©íŠ¸ ê²€ì¶œ {metrics.facial_artifacts:.1f}ì ì´ ì¢…í•©ì ìœ¼ë¡œ ê²€í† ë˜ì—ˆìŠµë‹ˆë‹¤. ë³¸ ê°ì • ê²°ê³¼ëŠ” í˜„í–‰ ë””ì§€í„¸ í¬ë Œì‹ ê¸°ì¤€ì— ë¶€í•©í•˜ëŠ” ê²ƒìœ¼ë¡œ íŒë‹¨ë©ë‹ˆë‹¤."""


@app.get("/")
async def root():
    """
    ë£¨íŠ¸ ì—”ë“œí¬ì¸íŠ¸ - API ì •ë³´ ì œê³µ
    """
    return {
        "service": "DeepSentinel AI Server",
        "version": "2.1.0",
        "status": "running",
        "features": ["deepfake_detection", "gpt_report_generation", "realtime_analysis"]
    }


@app.get("/api/health")
async def health_check():
    """
    í—¬ìŠ¤ ì²´í¬ ì—”ë“œí¬ì¸íŠ¸
    """
    openai_status = "configured" if os.getenv("OPENAI_API_KEY") else "not_configured"
    
    return {
        "status": "healthy",
        "timestamp": time.time(),
        "openai": openai_status,
        "websocket": "enabled"
    }


@app.post("/api/analyze", response_model=AnalysisResponse)
async def analyze_video(video: UploadFile = File(...)):
    """
    ì˜ìƒ ë”¥í˜ì´í¬ ë¶„ì„ ì—”ë“œí¬ì¸íŠ¸
    
    Args:
        video: ë¶„ì„í•  ì˜ìƒ íŒŒì¼
        
    Returns:
        AnalysisResponse: ë¶„ì„ ê²°ê³¼ ë° GPT ë¦¬í¬íŠ¸
    """
    start_time = time.time()
    
    try:
        # íŒŒì¼ íƒ€ì… ê²€ì¦
        if not video.content_type or not video.content_type.startswith("video/"):
            raise HTTPException(
                status_code=400,
                detail="Invalid file type. Only video files are supported."
            )
        
        # ì„ì‹œ íŒŒì¼ ì½ê¸° (ì‹¤ì œ ë¶„ì„ ì‹œë®¬ë ˆì´ì…˜)
        contents = await video.read()
        file_size_mb = len(contents) / (1024 * 1024)
        
        print(f"ğŸ“¹ Analyzing video: {video.filename} ({file_size_mb:.2f} MB)")
        
        # ë¶„ì„ ì‹œë®¬ë ˆì´ì…˜ (ì§§ì€ ëŒ€ê¸°)
        await asyncio.sleep(0.5)
        
        # 1. ë©”íŠ¸ë¦­ ìƒì„±
        metrics = generate_analysis_metrics()
        
        # 2. ì¢…í•© ì ìˆ˜ ê³„ì‚°
        result, confidence = calculate_overall_score(metrics)
        
        # 3. GPT ë¦¬í¬íŠ¸ ìƒì„±
        report = await generate_gpt_report(metrics, result, confidence)
        
        # ë¶„ì„ ì†Œìš” ì‹œê°„
        analysis_time = time.time() - start_time
        
        print(f"âœ… Analysis complete: {result} (confidence: {confidence:.2%})")
        
        return AnalysisResponse(
            result=result,
            confidence=confidence,
            metrics=metrics,
            report=report,
            analysis_time=analysis_time
        )
        
    except HTTPException:
        raise
    except Exception as e:
        print(f"âŒ Analysis error: {str(e)}")
        raise HTTPException(
            status_code=500,
            detail=f"Analysis failed: {str(e)}"
        )


@app.post("/api/analyze-url")
async def analyze_url(request: URLAnalysisRequest):
    """
    URL ê¸°ë°˜ ì˜ìƒ ë¶„ì„ ì—”ë“œí¬ì¸íŠ¸ (ìŠ¤ì¼ˆë ˆí†¤)
    
    Args:
        request: URL ë¶„ì„ ìš”ì²­ (ìœ íŠœë¸Œ, ë¦´ìŠ¤ ë“±)
        
    Returns:
        dict: ë¶„ì„ ê²°ê³¼ (ë”ë¯¸)
    """
    print(f"ğŸ”— Analyzing URL: {request.url}")
    
    # ë”ë¯¸ ì‘ë‹µ (í–¥í›„ youtube-dl, yt-dlp ë“±ìœ¼ë¡œ êµ¬í˜„)
    await asyncio.sleep(1)
    
    return {
        "status": "success",
        "message": "URL ë¶„ì„ì€ í˜„ì¬ ê°œë°œ ì¤‘ì…ë‹ˆë‹¤.",
        "url": request.url,
        "platform": request.platform or "unknown"
    }


@app.websocket("/ws/analyze")
async def websocket_analyze(websocket: WebSocket):
    """
    ì‹¤ì‹œê°„ í”„ë ˆì„ ë¶„ì„ìš© WebSocket ì—”ë“œí¬ì¸íŠ¸
    
    Usage:
        í´ë¼ì´ì–¸íŠ¸ê°€ ì›¹ìº  í”„ë ˆì„ì„ base64ë¡œ ì¸ì½”ë”©í•˜ì—¬ ì „ì†¡í•˜ë©´
        MediaPipeë¡œ ì‹¤ì‹œê°„ ë¶„ì„ ê²°ê³¼ë¥¼ ìŠ¤íŠ¸ë¦¬ë°
    """
    await websocket.accept()
    print("ğŸ”Œ WebSocket connection established")
    
    # FaceAnalyzer ì´ˆê¸°í™”
    try:
        from face_analyzer import FaceAnalyzer
        import base64
        import cv2
        
        analyzer = FaceAnalyzer()
        frame_skip = 0  # í”„ë ˆì„ ìŠ¤í‚µ ì¹´ìš´í„° (ì§€ì—° ì‹œê°„ ê°ì†Œìš©)
        
        while True:
            # í´ë¼ì´ì–¸íŠ¸ë¡œë¶€í„° í”„ë ˆì„ ìˆ˜ì‹ 
            data = await websocket.receive_text()
            frame_data = json.loads(data)
            
            # í”„ë ˆì„ ìŠ¤í‚µ ë¡œì§ (2í”„ë ˆì„ë§ˆë‹¤ 1ë²ˆì”© ë¶„ì„)
            frame_skip += 1
            if frame_skip % 2 != 0:
                continue
            
            try:
                # Base64 ë””ì½”ë”©
                img_data = base64.b64decode(frame_data.get('frame', ''))
                nparr = np.frombuffer(img_data, np.uint8)
                frame = cv2.imdecode(nparr, cv2.IMREAD_COLOR)
                
                if frame is None:
                    await websocket.send_json({
                        "error": "Invalid frame data",
                        "timestamp": time.time()
                    })
                    continue
                
                # MediaPipe ì–¼êµ´ ë¶„ì„
                analysis_result = analyzer.analyze_frame(frame)
                
                if analysis_result is None:
                    # ì–¼êµ´ ë¯¸ê²€ì¶œ
                    await websocket.send_json({
                        "face_detected": False,
                        "timestamp": time.time(),
                        "message": "ì–¼êµ´ì´ ê°ì§€ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤"
                    })
                    continue
                
                # ì¢…í•© ì ìˆ˜ ê³„ì‚°
                metrics_obj = AnalysisMetrics(
                    eye_blink_rate=analysis_result['eye_blink_rate'],
                    lip_sync_score=analysis_result['lip_sync_score'],
                    lighting_consistency=analysis_result['lighting_consistency'],
                    facial_artifacts=analysis_result['facial_artifacts'],
                    texture_quality=analysis_result['texture_quality'],
                    motion_smoothness=analysis_result['motion_smoothness']
                )
                
                result, confidence = calculate_overall_score(metrics_obj)
                
                # ê²°ê³¼ ì „ì†¡
                response = {
                    "timestamp": time.time(),
                    "result": result,
                    "confidence": confidence,
                    "face_detected": True,
                    "metrics": {
                        "eye_blink_rate": round(analysis_result['eye_blink_rate'], 1),
                        "lip_sync_score": round(analysis_result['lip_sync_score'], 1),
                        "lighting_consistency": round(analysis_result['lighting_consistency'], 1),
                        "facial_artifacts": round(analysis_result['facial_artifacts'], 1),
                        "texture_quality": round(analysis_result['texture_quality'], 1),
                        "motion_smoothness": round(analysis_result['motion_smoothness'], 1),
                    },
                    "details": {
                        "ear": round(analysis_result.get('ear', 0), 3),
                        "mar": round(analysis_result.get('mar', 0), 3),
                        "angles": {
                            "pitch": round(analysis_result['angles']['pitch'], 1),
                            "yaw": round(analysis_result['angles']['yaw'], 1),
                            "roll": round(analysis_result['angles']['roll'], 1)
                        }
                    }
                }
                
                await websocket.send_json(response)
                print(f"ğŸ“¡ Sent realtime analysis: {result} ({confidence:.2%})")
                
            except Exception as e:
                print(f"âš ï¸ Frame analysis error: {str(e)}")
                await websocket.send_json({
                    "error": str(e),
                    "timestamp": time.time()
                })
                
    except WebSocketDisconnect:
        print("ğŸ”Œ WebSocket connection closed")
    except Exception as e:
        print(f"âŒ WebSocket error: {str(e)}")
        await websocket.close()
    finally:
        # ë¶„ì„ê¸° ì •ë¦¬
        if 'analyzer' in locals():
            analyzer.reset()


if __name__ == "__main__":
    port = int(os.getenv("AI_SERVER_PORT", 8000))
    uvicorn.run(
        "main:app",
        host="0.0.0.0",
        port=port,
        reload=True
    )
